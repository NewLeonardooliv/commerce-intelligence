# Agent Evaluation Framework

Este documento descreve o framework de avaliaÃ§Ã£o (evals) dos agents do Commerce Intelligence.

## ğŸ“‹ VisÃ£o Geral

O framework de evals permite testar e avaliar sistematicamente o desempenho dos agents de IA em diferentes cenÃ¡rios e casos de uso. Ele fornece mÃ©tricas objetivas para medir:

- **PrecisÃ£o**: O agent entende corretamente a intenÃ§Ã£o do usuÃ¡rio?
- **Qualidade**: As respostas sÃ£o relevantes, corretas e bem formatadas?
- **Robustez**: O agent lida bem com casos extremos e ambÃ­guos?
- **Performance**: Qual Ã© a velocidade de resposta?

## ğŸ—ï¸ Estrutura

```
src/tests/evals/
â”œâ”€â”€ types.ts                    # Tipos e interfaces do framework
â”œâ”€â”€ metrics.ts                  # FunÃ§Ãµes de cÃ¡lculo de mÃ©tricas
â”œâ”€â”€ runner.ts                   # Executor de evals (base class)
â”œâ”€â”€ cases/                      # Casos de teste organizados
â”‚   â”œâ”€â”€ interpreter.evals.ts    # Testes do Interpreter Agent
â”‚   â”œâ”€â”€ data-query.evals.ts     # Testes do Data Query Agent
â”‚   â”œâ”€â”€ responder.evals.ts      # Testes do Responder Agent
â”‚   â””â”€â”€ orchestrator.evals.ts   # Testes do Orchestrator completo
â”œâ”€â”€ evaluators/                 # Avaliadores especÃ­ficos por agent
â”‚   â”œâ”€â”€ interpreter.evaluator.ts
â”‚   â””â”€â”€ orchestrator.evaluator.ts
â””â”€â”€ index.test.ts               # Arquivo principal de testes
```

## ğŸš€ Como Executar

### Executar todos os testes

```bash
bun test src/tests/evals/
```

### Executar teste especÃ­fico

```bash
bun test src/tests/evals/index.test.ts
```

### Executar manualmente (modo desenvolvimento)

```bash
bun src/tests/evals/index.test.ts
```

### Executar com filtros de tags

```typescript
const runner = new InterpreterEvalRunner({
  tags: ['basic'],          // Apenas casos com tag 'basic'
  skipTags: ['slow'],       // Pular casos com tag 'slow'
  verbose: true,            // Mostrar detalhes
  stopOnFailure: false,     // Continuar mesmo com falhas
});
```

## ğŸ“ Estrutura de um Caso de Teste

```typescript
{
  id: 'interp-001',
  name: 'Simple product listing query',
  description: 'User asks for available products',
  input: {
    userQuery: 'Quais produtos temos disponÃ­veis?',
  },
  expected: {
    intentContains: ['produto', 'listar'],
    confidence: { min: 0.7 },
    requiresData: true,
  },
  tags: ['interpreter', 'basic', 'products'],
}
```

### Campos Principais

#### Input
- `userQuery`: Pergunta do usuÃ¡rio
- `sessionId`: (Opcional) ID da sessÃ£o
- `conversationHistory`: (Opcional) HistÃ³rico de mensagens

#### Expected
- **Para Interpreter Agent:**
  - `intent`: IntenÃ§Ã£o esperada (string ou regex)
  - `intentContains`: Palavras-chave que devem estar na intenÃ§Ã£o
  - `confidence`: NÃ­vel de confianÃ§a mÃ­nimo/mÃ¡ximo
  - `requiresData`: Se requer consulta ao banco
  - `entities`: Entidades que devem ser extraÃ­das

- **Para Data Query Agent:**
  - `sqlContains`: Palavras-chave que devem estar no SQL
  - `sqlNotContains`: Palavras que NÃƒO devem estar no SQL
  - `hasResults`: Se deve retornar resultados
  - `minResults`/`maxResults`: Quantidade de resultados

- **Para Responder Agent:**
  - `responseContains`: Palavras que devem estar na resposta
  - `responseNotContains`: Palavras que NÃƒO devem estar
  - `answersQuestion`: Se responde a pergunta
  - `inPortuguese`: Se resposta estÃ¡ em portuguÃªs
  - `responseLength`: Tamanho mÃ­nimo/mÃ¡ximo

- **Para Orchestrator:**
  - `completes`: Se o pipeline completa
  - `noErrors`: Se nÃ£o hÃ¡ erros
  - `agentsRun`: Quais agents devem executar
  - Todos os critÃ©rios acima combinados

## ğŸ“Š MÃ©tricas e Scoring

### Score de InterpretaÃ§Ã£o
Avalia a qualidade da interpretaÃ§Ã£o da intenÃ§Ã£o:
- ConfianÃ§a do AI (40%)
- Clareza da intenÃ§Ã£o (20%)
- Entidades identificadas (20%)
- Queries sugeridas (20%)

### Score de Qualidade de Resposta
Avalia a qualidade da resposta gerada:
- Resposta nÃ£o vazia e sem erros (20%)
- Responde Ã  pergunta (30%)
- EstÃ¡ em portuguÃªs (20%)
- ContÃ©m palavras-chave esperadas (15%)
- NÃ£o contÃ©m palavras indesejadas (10%)
- Tamanho adequado (5%)

### Score Final
- `score`: 0.0 a 1.0 (porcentagem de critÃ©rios atendidos)
- `passed`: true se score >= 0.7 e sem erros crÃ­ticos

## ğŸ“ˆ RelatÃ³rio de Resultados

Exemplo de saÃ­da:

```
ğŸ§ª Running 14 evaluation cases...

[1/14] Running: Complete simple query pipeline
  âœ… PASSED

[2/14] Running: Complete customer count pipeline
  âœ… PASSED

...

============================================================
ğŸ“Š EVALUATION SUMMARY
============================================================
Total Cases: 14
Passed: 12 âœ…
Failed: 2 âŒ
Average Score: 82.5%
Average Duration: 1250ms
============================================================
```

## ğŸ·ï¸ Tags DisponÃ­veis

### Por Agent
- `interpreter`, `data-query`, `responder`, `orchestrator`

### Por Complexidade
- `basic`: Casos simples e diretos
- `intermediate`: Casos de complexidade mÃ©dia
- `advanced`: Casos complexos e desafiadores

### Por Categoria
- `products`, `customers`, `orders`, `revenue`, `payments`, `reviews`
- `aggregation`, `joins`, `ranking`, `temporal`, `geography`

### Casos Especiais
- `edge-case`: Casos extremos
- `error-handling`: Testes de tratamento de erro
- `security`: Testes de seguranÃ§a (SQL injection, etc)
- `multilingual`: Testes com mÃºltiplos idiomas
- `context`: Testes com histÃ³rico de conversa

## ğŸ”§ Criando Novos Casos de Teste

1. Adicione o caso no arquivo apropriado em `cases/`
2. Use um ID Ãºnico e descritivo
3. Adicione tags relevantes
4. Defina critÃ©rios de sucesso claros
5. Documente casos especiais

```typescript
export const myNewEvalCases: EvalCase[] = [
  {
    id: 'custom-001',
    name: 'My custom test',
    description: 'What this test evaluates',
    input: {
      userQuery: 'Your test query',
    },
    expected: {
      // Your expectations
    },
    tags: ['custom', 'basic'],
    timeout: 30000, // Optional custom timeout
  },
];
```

## ğŸ¯ Melhores PrÃ¡ticas

1. **Cobertura**: Teste casos comuns, edge cases e erros
2. **IndependÃªncia**: Cada teste deve ser independente
3. **Clareza**: Nome e descriÃ§Ã£o devem ser auto-explicativos
4. **Realismo**: Use queries reais que usuÃ¡rios fariam
5. **ManutenÃ§Ã£o**: Revise e atualize regularmente os evals

## ğŸ› Debug e Troubleshooting

### Ver detalhes dos testes

```typescript
const runner = new OrchestratorEvalRunner({ verbose: true });
```

### Executar apenas um caso

```typescript
const singleCase = orchestratorEvalCases.find(c => c.id === 'orch-001');
const result = await runner.run([singleCase]);
```

### Analisar falhas

O objeto `EvalResult` contÃ©m:
- `errors`: Erros crÃ­ticos que causaram falha
- `warnings`: Avisos que nÃ£o impedem sucesso
- `output`: Dados de saÃ­da do agent para inspeÃ§Ã£o
- `metrics`: MÃ©tricas calculadas

## ğŸ“š Exemplos de Uso

### Benchmark de Performance

```typescript
const runner = new OrchestratorEvalRunner();
const results = await runner.run(orchestratorEvalCases);

console.log(`Avg Duration: ${results.averageDuration}ms`);
console.log(`P95 Duration: ${calculateP95(results.results.map(r => r.duration))}ms`);
```

### CI/CD Integration

```typescript
// No seu pipeline de CI
const runner = new OrchestratorEvalRunner({
  stopOnFailure: true,  // Falha rÃ¡pida
  tags: ['basic'],       // Apenas testes bÃ¡sicos
});

const summary = await runner.run(orchestratorEvalCases);

if (summary.averageScore < 0.8) {
  process.exit(1);  // Falha o build se score < 80%
}
```

### Regression Testing

```typescript
// Salve resultados baseline
const baseline = await runner.run(allCases);
saveResults('baseline.json', baseline);

// Compare com nova versÃ£o
const current = await runner.run(allCases);
const regression = compareResults(baseline, current);

if (regression.scoreDropped > 0.1) {
  console.warn('âš ï¸  Performance regression detected!');
}
```

## ğŸ”„ Continuous Improvement

Os evals devem evoluir com o produto:

1. Adicione casos para bugs reportados
2. Adicione casos para novas features
3. Remova/atualize casos obsoletos
4. Aumente a cobertura gradualmente
5. Monitore mÃ©tricas ao longo do tempo

## ğŸ“– ReferÃªncias

- [OpenAI Evals](https://github.com/openai/evals)
- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation)
- [Agent Testing Best Practices](https://www.anthropic.com/index/testing-ai-agents)
