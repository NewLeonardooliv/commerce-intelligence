# Arquitetura do Framework de Evals

Este documento descreve a arquitetura e o fluxo de execuÃ§Ã£o do framework de avaliaÃ§Ã£o dos agents.

## ğŸ—ï¸ VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AGENT EVALUATION FRAMEWORK                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test Cases  â”‚â”€â”€â”€â”€â–¶â”‚   Evaluator  â”‚â”€â”€â”€â”€â–¶â”‚   Results    â”‚
â”‚  (*.evals.ts)â”‚     â”‚  (evaluate)  â”‚     â”‚  (metrics)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Reports    â”‚
                     â”‚ HTML/JSON/MD â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Componentes Principais

### 1. Test Cases (Casos de Teste)

```
cases/
â”œâ”€â”€ interpreter.evals.ts      # Casos para Interpreter Agent
â”œâ”€â”€ data-query.evals.ts       # Casos para Data Query Agent
â”œâ”€â”€ responder.evals.ts        # Casos para Responder Agent
â””â”€â”€ orchestrator.evals.ts     # Casos End-to-End
```

**Responsabilidades:**
- Definir entradas (user queries)
- Definir expectativas (expected outcomes)
- Organizar por tags e complexidade
- Documentar casos especÃ­ficos

**Estrutura:**
```typescript
{
  id: 'unique-id',
  name: 'Test name',
  input: { userQuery, sessionId?, conversationHistory? },
  expected: { intent?, sql?, response?, ... },
  tags: ['tag1', 'tag2']
}
```

### 2. Evaluators (Avaliadores)

```
evaluators/
â”œâ”€â”€ interpreter.evaluator.ts   # Avalia interpretaÃ§Ãµes
â””â”€â”€ orchestrator.evaluator.ts  # Avalia pipeline completo
```

**Responsabilidades:**
- Executar agents com casos de teste
- Coletar outputs
- Calcular mÃ©tricas
- Comparar com expectativas
- Gerar resultado (pass/fail + score)

**Interface:**
```typescript
interface IEvaluator {
  name: string;
  description: string;
  evaluate(testCase: EvalCase): Promise<EvalResult>;
  validateCase(testCase: EvalCase): boolean;
}
```

### 3. Metrics (MÃ©tricas)

```
metrics.ts
```

**Responsabilidades:**
- Calcular similaridade
- Validar respostas
- Detectar idioma
- Verificar seguranÃ§a SQL
- Calcular scores agregados

**FunÃ§Ãµes principais:**
```typescript
calculateSimilarity(str1, str2): number
answersQuestion(query, response): boolean
isPortuguese(text): boolean
isSafeSql(sql): boolean
calculateInterpretationScore(context): number
calculateResponseQuality(query, response): number
```

### 4. Runner (Executor)

```
runner.ts
```

**Responsabilidades:**
- Orquestrar execuÃ§Ã£o dos casos
- Controlar paralelismo
- Gerenciar timeouts
- Filtrar por tags
- Gerar sumÃ¡rio final

**Modos de execuÃ§Ã£o:**
- Sequential: Um caso por vez
- Parallel: MÃºltiplos casos simultaneamente

### 5. Report Generator (Gerador de RelatÃ³rios)

```
report-generator.ts
```

**Responsabilidades:**
- Gerar relatÃ³rio HTML (interativo)
- Gerar relatÃ³rio JSON (programÃ¡tico)
- Gerar relatÃ³rio Markdown (documentaÃ§Ã£o)
- Comparar com baselines
- Detectar regressÃµes

### 6. CLI (Interface de Linha de Comando)

```
cli.ts
```

**Responsabilidades:**
- Interface amigÃ¡vel para executar evals
- Filtros por tags e agents
- OpÃ§Ãµes de configuraÃ§Ã£o
- GeraÃ§Ã£o de relatÃ³rios sob demanda

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### Fluxo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Start     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Test Cases         â”‚
â”‚ - Read from cases/*.ts  â”‚
â”‚ - Filter by tags        â”‚
â”‚ - Validate cases        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For Each Test Case      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Agent(s)        â”‚
â”‚ - Run agent.process()   â”‚
â”‚ - Capture output        â”‚
â”‚ - Measure duration      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Calculate Metrics       â”‚
â”‚ - Intent accuracy       â”‚
â”‚ - SQL quality           â”‚
â”‚ - Response quality      â”‚
â”‚ - Overall score         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compare with Expected   â”‚
â”‚ - Check all criteria    â”‚
â”‚ - Generate errors list  â”‚
â”‚ - Generate warnings     â”‚
â”‚ - Determine pass/fail   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Result           â”‚
â”‚ - Score (0-1)           â”‚
â”‚ - Passed (bool)         â”‚
â”‚ - Duration (ms)         â”‚
â”‚ - Errors/Warnings       â”‚
â”‚ - Output data           â”‚
â”‚ - Metrics               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aggregate Results       â”‚
â”‚ - Total cases           â”‚
â”‚ - Passed/Failed count   â”‚
â”‚ - Average score         â”‚
â”‚ - Average duration      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate Reports        â”‚
â”‚ - HTML (interactive)    â”‚
â”‚ - JSON (data)           â”‚
â”‚ - Markdown (docs)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     End     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de AvaliaÃ§Ã£o (Detalhado)

```
Test Case
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           INTERPRETER EVALUATOR                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Run InterpreterAgent.process(context)        â”‚
â”‚ 2. Get interpretation result                    â”‚
â”‚ 3. Check:                                       â”‚
â”‚    âœ“ Intent matches expected                    â”‚
â”‚    âœ“ Confidence in range                        â”‚
â”‚    âœ“ RequiresData correct                       â”‚
â”‚    âœ“ Entities extracted                         â”‚
â”‚    âœ“ Language is Portuguese                     â”‚
â”‚ 4. Calculate score (0-1)                        â”‚
â”‚ 5. Return EvalResult                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                   [Result]


Test Case
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ORCHESTRATOR EVALUATOR                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Run Orchestrator.process(query)              â”‚
â”‚ 2. Get full context with all agent outputs      â”‚
â”‚ 3. Check:                                       â”‚
â”‚    âœ“ Pipeline completed                         â”‚
â”‚    âœ“ No errors in history                       â”‚
â”‚    âœ“ Expected agents ran                        â”‚
â”‚    âœ“ SQL is safe and correct                    â”‚
â”‚    âœ“ Results returned                           â”‚
â”‚    âœ“ Response quality                           â”‚
â”‚    âœ“ Suggestions generated                      â”‚
â”‚ 4. Calculate combined score                     â”‚
â”‚ 5. Return EvalResult                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                   [Result]
```

## ğŸ¯ Scoring System

### Score Calculation

```
Final Score = Î£(criterion_score) / total_criteria

Where each criterion is scored 0-1:
- 1.0 = Perfect match
- 0.7-0.99 = Good match
- 0.5-0.69 = Partial match
- 0-0.49 = Poor match
```

### Pass/Fail Logic

```
PASSED if:
  - Final Score >= 0.7 AND
  - No critical errors

FAILED if:
  - Final Score < 0.7 OR
  - Has critical errors
```

### Example Scoring

```typescript
// Interpreter Agent
maxScore = 0
score = 0

// Check intent (weight: 1)
if (intentMatches) score += 1
maxScore += 1

// Check confidence (weight: 1)
if (confidenceInRange) score += 1
maxScore += 1

// Check requiresData (weight: 1)
if (requiresDataCorrect) score += 1
maxScore += 1

// Check entities (weight: 1)
score += (matchedEntities / totalEntities)
maxScore += 1

// Final score
finalScore = score / maxScore
// Example: 3.5 / 4 = 0.875 = 87.5%
```

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Case    â”‚
â”‚ {            â”‚
â”‚   input,     â”‚
â”‚   expected   â”‚
â”‚ }            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent(s)     â”‚
â”‚ - Process    â”‚
â”‚ - Transform  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent Output â”‚
â”‚ {            â”‚
â”‚   context,   â”‚
â”‚   response,  â”‚
â”‚   data       â”‚
â”‚ }            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluator    â”‚
â”‚ - Measure    â”‚
â”‚ - Compare    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Eval Result  â”‚
â”‚ {            â”‚
â”‚   score,     â”‚
â”‚   passed,    â”‚
â”‚   errors     â”‚
â”‚ }            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Report       â”‚
â”‚ - Aggregate  â”‚
â”‚ - Format     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”Œ Extension Points

### 1. Adicionar Novo Agent

```typescript
// 1. Create evaluator
class MyAgentEvaluator implements IEvaluator {
  async evaluate(testCase: EvalCase): Promise<EvalResult> {
    // Your evaluation logic
  }
}

// 2. Create test cases
export const myAgentEvalCases: EvalCase[] = [
  // Your cases
];

// 3. Add to index.test.ts
import { myAgentEvalCases } from './cases/my-agent.evals';
```

### 2. Adicionar Nova MÃ©trica

```typescript
// In metrics.ts
export function myCustomMetric(data: any): number {
  // Calculate your metric (0-1)
  return score;
}

// Use in evaluator
const customScore = myCustomMetric(context.data);
```

### 3. Adicionar Novo Formato de RelatÃ³rio

```typescript
// In report-generator.ts
export async function generateXmlReport(
  summary: EvalSummary,
  outputPath?: string
): Promise<string> {
  // Generate XML format
}
```

## ğŸ¨ Design Patterns

### 1. Strategy Pattern
- Different evaluators for different agents
- Pluggable metric calculators
- Multiple report formats

### 2. Template Method Pattern
- `EvalRunner` base class
- Subclasses implement `runCase()`
- Common flow in base class

### 3. Builder Pattern
- Test cases built incrementally
- Flexible configuration
- Optional parameters

### 4. Observer Pattern
- Progress callbacks
- Event logging
- Real-time updates

## ğŸ”’ Best Practices

### Test Cases
- âœ… Unique IDs
- âœ… Descriptive names
- âœ… Clear expectations
- âœ… Appropriate tags
- âœ… Independent tests

### Evaluators
- âœ… Single responsibility
- âœ… Composable metrics
- âœ… Error handling
- âœ… Performance tracking
- âœ… Detailed feedback

### Reports
- âœ… Multiple formats
- âœ… Clear visualization
- âœ… Actionable insights
- âœ… Historical comparison
- âœ… Easy sharing

### CI/CD
- âœ… Fast feedback (< 5min)
- âœ… Fail fast on errors
- âœ… Archive results
- âœ… Trend analysis
- âœ… Automated alerts

## ğŸ“š References

- [OpenAI Evals](https://github.com/openai/evals)
- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation)
- [Martin Fowler - Testing](https://martinfowler.com/testing/)
- [Google Testing Blog](https://testing.googleblog.com/)

---

**Ãšltima atualizaÃ§Ã£o:** 2026-01-28  
**VersÃ£o:** 1.0.0
