# Agent Evaluation Framework

Framework completo para avaliar a performance dos agents de IA do Commerce Intelligence.

## ğŸ“ Estrutura

```
src/tests/evals/
â”œâ”€â”€ README.md                      # Este arquivo
â”œâ”€â”€ QUICKSTART.md                  # Guia rÃ¡pido de inÃ­cio
â”œâ”€â”€ EXAMPLE.md                     # Exemplos detalhados de como criar testes
â”œâ”€â”€ types.ts                       # Tipos e interfaces TypeScript
â”œâ”€â”€ metrics.ts                     # FunÃ§Ãµes de cÃ¡lculo de mÃ©tricas
â”œâ”€â”€ runner.ts                      # Classe base para executores de eval
â”œâ”€â”€ report-generator.ts            # Gerador de relatÃ³rios (HTML, JSON, MD)
â”œâ”€â”€ cli.ts                         # CLI para execuÃ§Ã£o de evals
â”œâ”€â”€ index.test.ts                  # Arquivo principal de testes
â”œâ”€â”€ cases/                         # Casos de teste organizados
â”‚   â”œâ”€â”€ interpreter.evals.ts       # 12 casos para Interpreter Agent
â”‚   â”œâ”€â”€ data-query.evals.ts        # 12 casos para Data Query Agent
â”‚   â”œâ”€â”€ responder.evals.ts         # 12 casos para Responder Agent
â”‚   â””â”€â”€ orchestrator.evals.ts      # 14 casos para Orchestrator (E2E)
â””â”€â”€ evaluators/                    # Avaliadores especÃ­ficos
    â”œâ”€â”€ interpreter.evaluator.ts   # Avaliador do Interpreter
    â””â”€â”€ orchestrator.evaluator.ts  # Avaliador do Orchestrator (E2E)
```

## ğŸš€ InÃ­cio RÃ¡pido

```bash
# Executar todos os evals
bun run test:evals

# Gerar relatÃ³rios
bun run evals:report

# Ver ajuda
bun src/tests/evals/cli.ts --help
```

## ğŸ“Š EstatÃ­sticas

- **Total de casos**: 50+ casos de teste
- **Agentes cobertos**: 5 agents (Interpreter, DataQuery, Responder, Suggestion, Orchestrator)
- **Categorias**: 15+ categorias (products, customers, orders, etc)
- **Complexidade**: bÃ¡sico, intermediÃ¡rio, avanÃ§ado
- **Tipos**: funcional, integraÃ§Ã£o, E2E, seguranÃ§a, edge cases

## ğŸ“– DocumentaÃ§Ã£o

- **[QUICKSTART.md](./QUICKSTART.md)**: Comece aqui! Guia rÃ¡pido de 5 minutos
- **[EXAMPLE.md](./EXAMPLE.md)**: Exemplos completos de como criar novos casos
- **[README-EVALS.md](../../../README-EVALS.md)**: DocumentaÃ§Ã£o completa do framework

## ğŸ¯ Casos de Teste

### Interpreter Agent (12 casos)

Testa a capacidade de interpretar queries e extrair intenÃ§Ãµes:

- âœ… Queries simples (produtos, clientes, faturamento)
- âœ… Queries complexas (agregaÃ§Ãµes, mÃºltiplas entidades)
- âœ… Queries ambÃ­guas
- âœ… Queries multilÃ­ngues
- âœ… ExtraÃ§Ã£o de entidades
- âœ… DetecÃ§Ã£o de ferramentas externas

### Data Query Agent (12 casos)

Testa a geraÃ§Ã£o de SQL e consultas ao banco:

- âœ… SQL bÃ¡sico (SELECT, WHERE, GROUP BY)
- âœ… Joins e relacionamentos
- âœ… AgregaÃ§Ãµes (COUNT, SUM, AVG)
- âœ… Rankings e ordenaÃ§Ãµes
- âœ… Filtros temporais
- âœ… SeguranÃ§a (prevenÃ§Ã£o de SQL injection)
- âœ… TraduÃ§Ãµes de categorias

### Responder Agent (12 casos)

Testa a qualidade das respostas geradas:

- âœ… Respostas contextuais
- âœ… Respostas em portuguÃªs
- âœ… Uso de dados numÃ©ricos
- âœ… SumarizaÃ§Ã£o de resultados
- âœ… Tom conversacional
- âœ… Tratamento de dados vazios
- âœ… PrecisÃ£o das informaÃ§Ãµes

### Orchestrator (14 casos E2E)

Testa o pipeline completo de agents:

- âœ… Fluxo completo (interpretaÃ§Ã£o â†’ query â†’ resposta)
- âœ… CoordenaÃ§Ã£o entre agents
- âœ… Tratamento de erros
- âœ… Contexto conversacional
- âœ… Queries complexas multi-tabela
- âœ… AnÃ¡lises temporais e geogrÃ¡ficas
- âœ… GeraÃ§Ã£o de sugestÃµes

## ğŸ·ï¸ Tags DisponÃ­veis

### Por Agent
- `interpreter`, `data-query`, `responder`, `suggestion`, `orchestrator`

### Por Complexidade
- `basic`: Casos simples e diretos
- `intermediate`: Complexidade mÃ©dia
- `advanced`: Casos complexos e desafiadores

### Por Categoria
- `products`, `customers`, `orders`, `sellers`, `payments`, `reviews`
- `aggregation`, `joins`, `ranking`, `filtering`, `temporal`, `geography`

### Casos Especiais
- `edge-case`: Casos extremos
- `error-handling`: Tratamento de erros
- `security`: Testes de seguranÃ§a
- `multilingual`: MÃºltiplos idiomas
- `context`: Com histÃ³rico de conversa

## ğŸ“ˆ MÃ©tricas Avaliadas

### Score de InterpretaÃ§Ã£o (0-1)
- ConfianÃ§a do AI (40%)
- Clareza da intenÃ§Ã£o (20%)
- Entidades identificadas (20%)
- Queries sugeridas (20%)

### Score de Qualidade de SQL (0-1)
- SQL seguro (30%)
- ContÃ©m keywords esperadas (50%)
- NÃ£o contÃ©m keywords proibidas (20%)

### Score de Qualidade de Resposta (0-1)
- Resposta nÃ£o vazia (20%)
- Responde Ã  pergunta (30%)
- EstÃ¡ em portuguÃªs (20%)
- ContÃ©m informaÃ§Ãµes esperadas (15%)
- Tamanho adequado (15%)

### Score Geral (0-1)
- MÃ©dia ponderada de todos os critÃ©rios
- **Passed**: score >= 0.7 e sem erros crÃ­ticos

## ğŸ› ï¸ Scripts DisponÃ­veis

```bash
# Via npm/bun scripts
bun run test:evals                    # Todos os evals via Bun Test
bun run test:evals:interpreter        # Apenas interpreter
bun run test:evals:orchestrator       # Apenas orchestrator
bun run test:evals:comprehensive      # Suite completa
bun run evals                         # Manual runner
bun run evals:cli                     # CLI tool
bun run evals:report                  # Gera todos os relatÃ³rios

# Comandos diretos
bun test src/tests/evals/             # Testes via Bun Test
bun src/tests/evals/index.test.ts     # Manual runner
bun src/tests/evals/cli.ts [options]  # CLI com opÃ§Ãµes
```

## ğŸ¨ Tipos de RelatÃ³rios

### HTML (Interativo)
- Formato visual e interativo
- Resultados expandÃ­veis por clique
- GrÃ¡ficos e estatÃ­sticas
- Ideal para: apresentaÃ§Ãµes, revisÃµes de equipe

### JSON (ProgramÃ¡tico)
- Formato estruturado e completo
- FÃ¡cil de processar programaticamente
- Ideal para: CI/CD, comparaÃ§Ãµes, analytics

### Markdown (DocumentaÃ§Ã£o)
- Formato texto legÃ­vel
- Pode ser commitado no git
- Ideal para: documentaÃ§Ã£o, PRs, READMEs

## ğŸ”„ Fluxo de AvaliaÃ§Ã£o

```
1. Carregar casos de teste
   â†“
2. Para cada caso:
   - Executar agent(s)
   - Capturar output
   - Calcular mÃ©tricas
   - Comparar com expectativas
   - Gerar resultado (pass/fail + score)
   â†“
3. Agregar resultados
   â†“
4. Gerar relatÃ³rio final
   â†“
5. Salvar relatÃ³rios (HTML, JSON, MD)
```

## ğŸ’¡ Casos de Uso

### 1. Desenvolvimento Local
```bash
# Teste rÃ¡pido apÃ³s mudanÃ§as
bun src/tests/evals/cli.ts --tags=basic --verbose
```

### 2. CI/CD Pipeline
```bash
# ValidaÃ§Ã£o automÃ¡tica
bun run test:evals
```

### 3. Benchmarking
```bash
# Medir performance ao longo do tempo
bun run evals:report
# Comparar com baselines anteriores
```

### 4. Debugging
```bash
# Isolar e investigar falhas
bun src/tests/evals/cli.ts --tags=edge-case --verbose --stopOnFailure
```

### 5. Regression Testing
```bash
# Garantir que mudanÃ§as nÃ£o quebram funcionalidades
bun test src/tests/evals/
```

## ğŸ“ Como Contribuir

### Adicionar Novos Casos

1. Escolha o arquivo apropriado em `cases/`
2. Siga os exemplos existentes
3. Use tags apropriadas
4. Teste localmente
5. Abra um PR

Veja [EXAMPLE.md](./EXAMPLE.md) para guia completo.

### Criar Novo Evaluator

1. Implemente a interface `IEvaluator`
2. Adicione em `evaluators/`
3. Integre no `index.test.ts`
4. Documente no README

### Melhorar MÃ©tricas

1. Edite `metrics.ts`
2. Adicione testes para suas funÃ§Ãµes
3. Atualize documentaÃ§Ã£o

## ğŸ“Š Resultados Esperados

### Interpreter Agent
- Pass rate: **80%+**
- Average score: **85%+**
- Avg duration: **500-1000ms**

### Data Query Agent
- Pass rate: **75%+**
- Average score: **80%+**
- Avg duration: **1000-2000ms**

### Responder Agent
- Pass rate: **85%+**
- Average score: **85%+**
- Avg duration: **800-1500ms**

### Orchestrator (E2E)
- Pass rate: **70%+**
- Average score: **75%+**
- Avg duration: **2000-4000ms**

## ğŸ› Troubleshooting

### Testes falhando
1. Execute com `--verbose` para ver detalhes
2. Verifique se expectativas sÃ£o realistas
3. Teste manualmente o mesmo query
4. Ajuste thresholds se necessÃ¡rio

### Testes lentos
1. Use `--tags=basic` para testes rÃ¡pidos
2. Aumente timeout se necessÃ¡rio
3. Use `--parallel` (com cuidado)
4. Considere mocks para testes unitÃ¡rios

### Resultados inconsistentes
1. Verifique dependÃªncias entre testes
2. Use dados determinÃ­sticos
3. Considere variaÃ§Ãµes da IA
4. Ajuste confidence thresholds

## ğŸ”— Links Ãšteis

- [OpenAI Evals](https://github.com/openai/evals)
- [LangChain Evaluation](https://python.langchain.com/docs/guides/evaluation)
- [Anthropic Testing Guide](https://www.anthropic.com/index/testing-ai-agents)

## ğŸ“ Changelog

### v1.0.0 (2026-01-28)
- âœ¨ Framework inicial completo
- ğŸ“Š 50+ casos de teste
- ğŸ“ˆ MÃ©tricas de avaliaÃ§Ã£o
- ğŸ“„ GeraÃ§Ã£o de relatÃ³rios (HTML, JSON, MD)
- ğŸ”§ CLI tool
- ğŸ“š DocumentaÃ§Ã£o completa

## ğŸ“„ LicenÃ§a

MIT - Commerce Intelligence

## ğŸ¤ Suporte

Para dÃºvidas ou sugestÃµes:
1. Leia a documentaÃ§Ã£o completa
2. Veja os exemplos em `EXAMPLE.md`
3. Execute com `--verbose` para debug
4. Abra uma issue no repositÃ³rio

---

**Pronto para comeÃ§ar?** Execute:

```bash
bun run test:evals
```

ğŸš€ Happy Testing!
